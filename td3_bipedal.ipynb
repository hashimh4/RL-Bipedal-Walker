{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### References:\n",
        "# S. Fujimoto. TD3. https://github.com/sfujim/TD3/blob/master/TD3.py. [Online; accessed 09-Feb-2023]. 2018.\n",
        "# F. Hu. BipedelWalker. https://github.com/FranciscoHu17/BipedalWalker. [Online; accessed 09-Feb-2023]. 2021."
      ],
      "metadata": {
        "id": "jJfULCGtUg-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA38jtUgtZsG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install xvfb -y\n",
        "!pip install 'swig'\n",
        "!pip install 'pyglet==1.5.27'\n",
        "!pip install 'gym[box2d]==0.20.0'\n",
        "!pip install 'pyvirtualdisplay==3.0'\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import random\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 50 # videos can take a very long time to render so only do it every N episodes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRG1-ykDTjGf",
        "outputId": "5ba67795-512d-47fc-df4a-08ec41e444e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHtclV_30Re"
      },
      "source": [
        "**Reinforcement learning agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXNHP8_U-rn"
      },
      "outputs": [],
      "source": [
        "explore_policy = 0.1\n",
        "alpha = .001\n",
        "policy_delay = 2\n",
        "tau = 0.005\n",
        "noise_policy = 0.2\n",
        "noise_clip = 0.5\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_actions):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        self.max_actions = max_actions\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_actions * torch.tanh(self.l3(x))\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l5 = nn.Linear(400, 300)\n",
        "        self.l6 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "\n",
        "        # Q1\n",
        "        c1 = F.relu(self.l1(sa))\n",
        "        c1 = F.relu(self.l2(c1))\n",
        "        c1 = self.l3(c1)\n",
        "\n",
        "        # Q2\n",
        "        c2 = F.relu(self.l4(sa))\n",
        "        c2 = F.relu(self.l5(c2))\n",
        "        c2 = self.l6(c2)\n",
        "        return (c1, c2)\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, buffer_size, batch_size, device):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size= batch_size\n",
        "        self.device = device\n",
        "        self.ptr = 0\n",
        "        print(self.buffer.maxlen)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        if self.ptr < self.buffer.maxlen:\n",
        "            self.buffer.append((state, action, reward, new_state, done))\n",
        "        else:\n",
        "            self.buffer[int(self.ptr)] = (state, action, reward, new_state, done)\n",
        "            self.ptr = (self.ptr + 1) % self.buffer.maxlen\n",
        "\n",
        "    def sample(self):\n",
        "        sample = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*sample)\n",
        "        states = torch.from_numpy(np.array(states, dtype=np.float32)).to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions, dtype=np.float32)).to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards, dtype=np.float32).reshape(-1, 1)).to(self.device)\n",
        "        next_states = torch.from_numpy(np.array(next_states, dtype=np.float32)).to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones, dtype=np.uint8).reshape(-1, 1)).float().to(self.device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, env, device):\n",
        "        super(Agent, self).__init__()\n",
        "\n",
        "        # Actor network\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=alpha)\n",
        "        self.device = device\n",
        "\n",
        "        # Critic network\n",
        "        self.critic = Critic(state_dim, action_dim).to(device) # only needs state + action\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=alpha)\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state, noise=0.1):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        if(noise == explore_policy):\n",
        "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), '/content/drive/MyDrive/td3_actor.pth')\n",
        "        torch.save(self.critic.state_dict(), '/content/drive/MyDrive/td3_critic.pth')\n",
        "        return\n",
        "\n",
        "    # def load(self):\n",
        "    #     self.actor.load_state_dict(torch.load(\"/content/drive/MyDrive/td3_actor.pth\",  map_location=torch.device('cpu')))\n",
        "    #     self.critic.load_state_dict(torch.load(\"/content/drive/MyDrive/td3_critic.pth\",  map_location=torch.device('cpu')))\n",
        "    #     return\n",
        "\n",
        "    def train(self, replay_buffer, current_iteration):\n",
        "        state, action, reward, next_state, done = replay_buffer.sample()\n",
        "\n",
        "        tensor_cpy = action.clone().detach()\n",
        "        noise = tensor_cpy.normal_(0, noise_policy).clamp(-noise_clip, noise_clip)\n",
        "\n",
        "        next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "        target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "        target_q = ((torch.min(target_q1, target_q2)) * (1-done)) + reward\n",
        "        curr_q1, curr_q2 = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(curr_q1, target_q) + F.mse_loss(curr_q2, target_q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        if (current_iteration % policy_delay == 0):\n",
        "            actor_loss = -self.critic(state, self.actor(state))[0].mean()\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "#env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
        "env = gym.wrappers.Monitor(env, \"/content/drive/MyDrive/video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUw4h980jfnu",
        "outputId": "78697d03-6076-45ed-81aa-2688e65ddb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The environment has 24 observations and the agent can take 4 actions\n",
            "The device is: cpu\n"
          ]
        }
      ],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDl6ViIDlVOk"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "buffer_size = 1000000\n",
        "batch_size = 100\n",
        "noise = 0.1\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(state_dim, action_dim, max_action, env, device)\n",
        "max_episodes = 1000\n",
        "max_timesteps = 2000\n",
        "\n",
        "try:\n",
        "    print(\"Loading previous model\")\n",
        "    agent.load()\n",
        "except Exception as e:\n",
        "    print('No previous model to load. Training from scratch.')\n",
        "\n",
        "buffer = ExperienceReplay(buffer_size, batch_size, device)\n",
        "\n",
        "save_score = 400\n",
        "episodes = 1000\n",
        "timesteps = 2000\n",
        "\n",
        "best_reward = -1*sys.maxsize\n",
        "scores_over_episodes = []\n",
        "\n",
        "# training procedure:\n",
        "for episode in range(1, max_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "\n",
        "        # select the agent action\n",
        "        action = agent.select_action(state) + np.random.normal(0, max_action * noise, size=action_dim)\n",
        "        action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # take action in environment and get r and s'\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        buffer.store_transition(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "        env.render()\n",
        "\n",
        "        # stop iterating when the episode finished\n",
        "        if(len(buffer) > batch_size):\n",
        "            agent.train(buffer, t)\n",
        "        if(done or t > timesteps):\n",
        "            scores_over_episodes.append(ep_reward)\n",
        "            print('Episode ', episode,'finished with reward:', ep_reward)\n",
        "            print('Finished at timestep ', t)\n",
        "            break\n",
        "\n",
        "\n",
        "    # append the episode reward to the reward list\n",
        "    reward_list.append(ep_reward)\n",
        "\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "    log_f.flush()\n",
        "    ep_reward = 0\n",
        "\n",
        "    if(np.mean(scores_over_episodes[-50:]) > save_score):\n",
        "        best_reward = np.mean(scores_over_episodes[-50:])\n",
        "        save_score = best_reward\n",
        "        agent.save()\n",
        "        break\n",
        "    if(episode >= 0 and ep_reward > best_reward):\n",
        "        best_reward = ep_reward\n",
        "        agent.save()\n",
        "\n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RezdZbZSYEEs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}